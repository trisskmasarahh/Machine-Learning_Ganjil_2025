{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhrc+fYZVwMK4QBxbjWgUv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trisskmasarahh/Machine-Learning_Ganjil_2025/blob/main/ANN_TRI_SUKMA_SARAH_3B_2341720051.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 1"
      ],
      "metadata": {
        "id": "WC9pyHkL_SCo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQCKVngn9Iwo",
        "outputId": "c4ff9451-4d6d-4a1f-9f79-cd0820818dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.26982240673410274\n",
            "Epoch 1000, Loss: 0.24956199103011312\n",
            "Epoch 2000, Loss: 0.23381478073581946\n",
            "Epoch 3000, Loss: 0.16860630285887235\n",
            "Epoch 4000, Loss: 0.06575747350298303\n",
            "Epoch 5000, Loss: 0.020225053567813383\n",
            "Epoch 6000, Loss: 0.010093171321889773\n",
            "Epoch 7000, Loss: 0.0064177580716371346\n",
            "Epoch 8000, Loss: 0.004616079042232019\n",
            "Epoch 9000, Loss: 0.003569522847902657\n",
            "Prediksi:\n",
            "[[0.04612863]\n",
            " [0.9487803 ]\n",
            " [0.94887809]\n",
            " [0.0648922 ]]\n"
          ]
        }
      ],
      "source": [
        "#Langkah:\n",
        "\n",
        "#Buat dataset sederhana (XOR).\n",
        "#Inisialisasi bobot dan bias.\n",
        "#Implementasikan forward pass.\n",
        "#Hitung error dan lakukan backpropagation.\n",
        "#Update bobot menggunakan gradient descent.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ubah jumlah neuron hidden layer menjadi 3."
      ],
      "metadata": {
        "id": "Hxtx2VbV_1_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tugas 1:\n",
        "\n",
        "#Ubah jumlah neuron hidden layer menjadi 3.\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3  # diganti dari 2 → 3 neuron\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Aktivasi sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "loss_history_3 = []\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    error = y - a2\n",
        "\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        loss_history_3.append(loss)\n",
        "        print(f\"[Hidden 3] Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "print(\"\\nPrediksi (hidden=3):\")\n",
        "print(a2.round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXlgKUPC-rOn",
        "outputId": "eeee00ba-a691-4e14-fe16-17f61c2ea485"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hidden 3] Epoch 0, Loss: 0.2542481957251896\n",
            "[Hidden 3] Epoch 1000, Loss: 0.25030913373915975\n",
            "[Hidden 3] Epoch 2000, Loss: 0.250075621030659\n",
            "[Hidden 3] Epoch 3000, Loss: 0.25000948248451094\n",
            "[Hidden 3] Epoch 4000, Loss: 0.24995747261221457\n",
            "[Hidden 3] Epoch 5000, Loss: 0.24986871625922982\n",
            "[Hidden 3] Epoch 6000, Loss: 0.2496137935926775\n",
            "[Hidden 3] Epoch 7000, Loss: 0.24831231699974896\n",
            "[Hidden 3] Epoch 8000, Loss: 0.2349810405742057\n",
            "[Hidden 3] Epoch 9000, Loss: 0.15184931217075504\n",
            "\n",
            "Prediksi (hidden=3):\n",
            "[[0.193]\n",
            " [0.852]\n",
            " [0.815]\n",
            " [0.164]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bandingkan hasil loss dengan konfigurasi awal."
      ],
      "metadata": {
        "id": "cpaIxsTIAKMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara keseluruhan, konfigurasi awal terbukti lebih superior dibandingkan konfigurasi dengan hidden layer 3. Konfigurasi awal mampu mencapai konvergensi yang lebih cepat dan menghasilkan nilai loss akhir yang jauh lebih kecil (0.003 berbanding 0.151). Hal ini menunjukkan bahwa arsitektur awal lebih mampu memetakan karakteristik data input dengan presisi tinggi tanpa mengalami hambatan belajar (stagnansi) seperti yang terjadi pada percobaan kedua."
      ],
      "metadata": {
        "id": "P07vrk_DF-iT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
      ],
      "metadata": {
        "id": "RZGJBVNrADOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tambahkan fungsi aktivasi ReLU dan bandingkan hasil.\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Aktivasi ReLU & Sigmoid\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "loss_history_relu = []\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    error = y - a2\n",
        "\n",
        "    # Backprop\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        loss_history_relu.append(loss)\n",
        "        print(f\"[ReLU] Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "print(\"\\nPrediksi (ReLU):\")\n",
        "print(a2.round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BdyioLm-15f",
        "outputId": "48183aa4-d0da-49a1-8ac6-ec25d925df35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ReLU] Epoch 0, Loss: 0.2800749885219016\n",
            "[ReLU] Epoch 1000, Loss: 0.25\n",
            "[ReLU] Epoch 2000, Loss: 0.25\n",
            "[ReLU] Epoch 3000, Loss: 0.25\n",
            "[ReLU] Epoch 4000, Loss: 0.25\n",
            "[ReLU] Epoch 5000, Loss: 0.25\n",
            "[ReLU] Epoch 6000, Loss: 0.25\n",
            "[ReLU] Epoch 7000, Loss: 0.25\n",
            "[ReLU] Epoch 8000, Loss: 0.25\n",
            "[ReLU] Epoch 9000, Loss: 0.25\n",
            "\n",
            "Prediksi (ReLU):\n",
            "[[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "perbandingan Loss Relu vs sigmoid"
      ],
      "metadata": {
        "id": "o0PqZLlbIYAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil pengujian memperlihatkan variasi signifikan pada loss akhir dan akurasi prediksi. Konfigurasi awal mencatatkan performa terbaik dengan penurunan loss yang drastis hingga 0.003, menghasilkan prediksi yang sangat presisi. Sementara itu, modifikasi jumlah neuron menjadi 3 (Hidden 3) menyebabkan penurunan performa, di mana loss akhir masih cukup tinggi (0.151) dan prediksi kurang meyakinkan.\n",
        "\n",
        "Penurunan performa paling ekstrim terjadi pada penggunaan fungsi aktivasi ReLU, di mana model gagal total dalam meminimalkan error. Loss tetap stagnan pada angka 0.25 sepanjang 9000 epoch, dan output prediksi rata di angka 0.5. Hal ini mengindikasikan bahwa arsitektur dengan ReLU pada konfigurasi ini tidak cocok untuk menyelesaikan permasalahan yang diujikan dibandingkan fungsi aktivasi Sigmoid pada konfigurasi awal"
      ],
      "metadata": {
        "id": "x1vGzKgXIgzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 2"
      ],
      "metadata": {
        "id": "oaTSel9rH0OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Langkah:\n",
        "\n",
        "#Import library.\n",
        "#Load dataset.\n",
        "#Bangun model.\n",
        "#Kompilasi dan latih model.\n",
        "#Evaluasi hasil.\n",
        "#Klasifikasi Data Iris\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi: {acc}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoEsYftLH5IG",
        "outputId": "6f5b2043-1bd1-4747-ef09-186e69c3b486"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3461 - loss: 3.2556\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2932 - loss: 2.7235 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3515 - loss: 2.2664 \n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3074 - loss: 1.9785 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3294 - loss: 1.5945 \n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3115 - loss: 1.4146 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3704 - loss: 1.1520 \n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3353 - loss: 1.1178 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3471 - loss: 0.9769 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6868 - loss: 0.9125 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5962 - loss: 0.9019 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5952 - loss: 0.8572 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6729 - loss: 0.8061 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7139 - loss: 0.8005 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7531 - loss: 0.7925 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7392 - loss: 0.8222 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7788 - loss: 0.7507 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8030 - loss: 0.7295 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8286 - loss: 0.6952 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8285 - loss: 0.7090 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8478 - loss: 0.6909 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8957 - loss: 0.7276 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9020 - loss: 0.7379 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9020 - loss: 0.6537 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9033 - loss: 0.6828 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8837 - loss: 0.6663 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8913 - loss: 0.6669 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8117 - loss: 0.6858 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8583 - loss: 0.6364 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8862 - loss: 0.6745 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8623 - loss: 0.6682 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8556 - loss: 0.6591\n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8929 - loss: 0.5724 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9189 - loss: 0.6215 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9142 - loss: 0.5860 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9283 - loss: 0.6186 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8868 - loss: 0.5761 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8566 - loss: 0.5894 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8921 - loss: 0.5858 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9118 - loss: 0.5767 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8786 - loss: 0.5283 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9021 - loss: 0.5442 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8519 - loss: 0.5483 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9034 - loss: 0.5975 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9313 - loss: 0.5181 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9200 - loss: 0.5045 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8968 - loss: 0.5886 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8762 - loss: 0.5993 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8830 - loss: 0.5410\n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9003 - loss: 0.5484\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9667 - loss: 0.5140\n",
            "Akurasi: 0.9666666388511658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tugas 2\n",
        "\n",
        "#Ubah jumlah neuron hidden layer.\n",
        "#Akurasi Model awal\n",
        "# Evaluasi\n",
        "print(f\"Akurasi Model Awal 10 -8 : {acc}\")\n",
        "\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi Model  8-4 : {acc}\")\n",
        "\n",
        "# model 16 -8\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi Model 16-8 : {acc}\")\n",
        "\n",
        "# model 32 - 16\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"AKurasi Model 32-16 : {acc}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5FZer7EKnip",
        "outputId": "cfd3382c-cdbd-4fa2-d2ac-ba86233a6477"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi Model Awal 10 -8 : 0.9666666388511658\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.9333 - loss: 0.3740\n",
            "Akurasi Model  8-4 : 0.9333333373069763\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 0.2817\n",
            "Akurasi Model 16-8 : 1.0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 0.1276\n",
            "AKurasi Model 32-16 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "perbandingan"
      ],
      "metadata": {
        "id": "bnDnpdc7MrP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil pengujian menunjukkan bahwa perubahan jumlah neuron pada hidden layer berpengaruh signifikan terhadap akurasi model. Dibandingkan dengan konfigurasi awal (10-8) yang mencapai akurasi 96,6%, pengurangan jumlah neuron menjadi 8-4 menyebabkan penurunan akurasi menjadi 93,3%.\n",
        "\n",
        "Sebaliknya, penambahan jumlah neuron terbukti sangat efektif. Kedua konfigurasi yang lebih besar, yaitu 16-8 dan 32-16, berhasil meningkatkan performa model secara maksimal hingga mencapai akurasi sempurna (100%). Hal ini mengindikasikan bahwa penambahan kompleksitas pada hidden layer membantu model mengenali pola data dengan jauh lebih baik."
      ],
      "metadata": {
        "id": "5AuUc5fMMty5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Tugas 3 — Perbandingan ReLU vs Sigmoid\n",
        "# ==============================\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model builder\n",
        "def build_model(n1, n2, activation):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(n1, activation=activation, input_shape=(4,)),\n",
        "        tf.keras.layers.Dense(n2, activation=activation),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# =======================\n",
        "# Model ReLU\n",
        "# =======================\n",
        "model_relu = build_model(10, 8, 'relu')\n",
        "model_relu.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "print(\"\\nEvaluasi Model ReLU 10-8\")\n",
        "loss_relu, acc_relu = model_relu.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Akurasi Model  relu-10-8 : {acc_relu}\")\n",
        "\n",
        "# =======================\n",
        "# Model Sigmoid\n",
        "# =======================\n",
        "model_sig = build_model(10, 8, 'sigmoid')\n",
        "model_sig.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "print(\"\\nEvaluasi Model Sigmoid 10-8\")\n",
        "loss_sig, acc_sig = model_sig.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Akurasi Model sigmoid : {acc_sig}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR5HOM4TMvim",
        "outputId": "e8202962-12be-45dc-b0aa-77fe6dbbe355"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluasi Model ReLU 10-8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 1.0000 - loss: 0.2860\n",
            "Akurasi Model  relu-10-8 : 1.0\n",
            "\n",
            "Evaluasi Model Sigmoid 10-8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.7333 - loss: 0.6003\n",
            "Akurasi Model sigmoid : 0.7333333492279053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Praktikum 3"
      ],
      "metadata": {
        "id": "E1PFpg0lM1lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Praktikum 3 kali ini kita akan mencoba menggunakan Keras untuk Regresi,\n",
        "# khususnya pada kasus Prediksi Harga Rumah.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Contoh dataset (buat dummy data)\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Evaluasi\n",
        "print(\"Prediksi:\", model.predict(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35kLvRoTM4Su",
        "outputId": "f749b8e2-c650-4bfb-9cba-160d9fede069"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746ms/step - loss: 1.5784\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.5671\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.5558\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.5446\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.5334\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.5224\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.5114\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.5005\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.4897\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.4789\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.4683\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.4577\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.4472\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.4368\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.4265\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.4162\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.4061\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.3960\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.3860\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.3761\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.3662\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.3565\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.3468\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.3373\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.3278\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.3183\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.3090\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.2998\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.2906\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.2815\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2725\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2636\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.2547\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.2460\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.2373\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.2287\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.2201\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2117\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.2033\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1951\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1872\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1794\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1717\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1641\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1565\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.1491\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.1417\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1343\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.1271\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1199\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.1128\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.1057\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0987\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0918\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0850\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0782\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0714\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0647\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0583\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.0524\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0467\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0409\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0353\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0299\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.0245\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0192\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.0139\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0087\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.0035\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9983\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.9932\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.9881\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9831\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9781\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.9731\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.9682\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.9633\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9585\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9537\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.9489\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.9441\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.9394\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.9347\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9301\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9255\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.9209\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.9164\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9119\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.9074\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.9029\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8985\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8941\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8898\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8855\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.8812\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.8769\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.8727\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8685\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.8643\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.8602\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Prediksi: [[-0.20307359]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Tugas 4:\n",
        "\n",
        "#Ubah learning rate.\n",
        "#Bandingkan hasil loss.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# ==========================\n",
        "# Dataset Dummy\n",
        "# ==========================\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# ==========================\n",
        "# Normalisasi\n",
        "# ==========================\n",
        "scalerX = StandardScaler()\n",
        "scalerY = StandardScaler()\n",
        "\n",
        "X = scalerX.fit_transform(X)\n",
        "y = scalerY.fit_transform(y)\n",
        "\n",
        "# ==========================\n",
        "# Split Data\n",
        "# ==========================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ==========================\n",
        "# Function untuk membuat model\n",
        "# ==========================\n",
        "def build_model(lr):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "# ==========================\n",
        "# TRAINING DENGAN 3 LEARNING RATE\n",
        "# ==========================\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.0001]\n",
        "histories = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\n===== Training dengan Learning Rate = {lr} =====\")\n",
        "    model = build_model(lr)\n",
        "    history = model.fit(X_train, y_train, epochs=100, verbose=0)\n",
        "    histories[lr] = history\n",
        "    print(f\"Loss Akhir (LR={lr}):\", history.history['loss'][-1])\n",
        "\n",
        "# ==========================\n",
        "# PREDIKSI DENGAN LR TERAKHIR (opsional)\n",
        "# ==========================\n",
        "model_last = build_model(0.001)\n",
        "model_last.fit(X_train, y_train, epochs=100, verbose=0)\n",
        "prediksi = model_last.predict(X_test)\n",
        "print(\"\\nPrediksi (dengan LR 0.001):\", prediksi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvWWl-YuM9Ma",
        "outputId": "a2f43fb8-b90a-43bc-d250-cb6e4151c234"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Training dengan Learning Rate = 0.001 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Akhir (LR=0.001): 0.5425208210945129\n",
            "\n",
            "===== Training dengan Learning Rate = 0.01 =====\n",
            "Loss Akhir (LR=0.01): 0.004486838821321726\n",
            "\n",
            "===== Training dengan Learning Rate = 0.0001 =====\n",
            "Loss Akhir (LR=0.0001): 1.2729132175445557\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\n",
            "Prediksi (dengan LR 0.001): [[-0.501438]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "perbandingan los"
      ],
      "metadata": {
        "id": "qHVPNOAxODDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil pengujian menunjukkan dampak signifikan learning rate terhadap performa model. Learning rate 0.01 menghasilkan loss akhir terendah yaitu 0.0044, yang mengindikasikan model berhasil belajar dengan sangat baik. Sebaliknya, learning rate yang lebih kecil (0.001 dan 0.0001) tidak mampu mencapai konvergensi optimal, dengan nilai loss yang masih sangat tinggi, masing-masing sebesar 0.5425 dan 1.2729."
      ],
      "metadata": {
        "id": "K2HnV53jOE-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Praktikum berikut akan menggunakan data Boston untuk memprediksi harga rumah.\n",
        "# MLP regresi (Keras)\n",
        "# ======================================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ======================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ======================================================\n",
        "# 2. LOAD BOSTON DATASET (MANUAL)\n",
        "# ======================================================\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "# X = 13 fitur\n",
        "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "\n",
        "# y = target housing price\n",
        "y = raw_df.values[1::2, 2]\n",
        "\n",
        "print(\"Shape X:\", X.shape)\n",
        "print(\"Shape y:\", y.shape)\n",
        "\n",
        "# ======================================================\n",
        "# 3. PREPROCESS (SCALING + SPLIT)\n",
        "# ======================================================\n",
        "scaler = StandardScaler()\n",
        "Xs = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    Xs, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# 4. BUILD MODEL\n",
        "# ======================================================\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mae'])\n",
        "\n",
        "# ======================================================\n",
        "# 5. TRAIN\n",
        "# ======================================================\n",
        "h = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# 6. PLOT TRAINING\n",
        "# ======================================================\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# Plot MSE\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(h.history['loss'], label='train_loss')\n",
        "plt.plot(h.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.title('MSE')\n",
        "\n",
        "# Plot MAE\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(h.history['mae'], label='train_mae')\n",
        "plt.plot(h.history['val_mae'], label='val_mae')\n",
        "plt.legend()\n",
        "plt.title('MAE')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ======================================================\n",
        "# 7. EVALUATE\n",
        "# ======================================================\n",
        "pred = model.predict(X_val)\n",
        "rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
        "print(\"RMSE:\", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "evtM7MK3OKBs",
        "outputId": "9648d588-4f79-40a6-d817-f2665ef6d17e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:22: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-1196433403.py:22: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape X: (506, 13)\n",
            "Shape y: (506,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "RMSE: 3.1587262926016373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS PRAKTIKUM"
      ],
      "metadata": {
        "id": "lADVHidiOQS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gunakan JST untuk klasifikasi angka tulisan tangan (MNIST).\n",
        "\n",
        "#Langkah:\n",
        "\n",
        "#Load dataset MNIST dari Keras.\n",
        "#Bangun model dengan 2 hidden layer.\n",
        "#Latih model dan evaluasi akurasi.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 → 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# -----------------------------\n",
        "# FUNGSI MEMBANGUN MODEL\n",
        "# -----------------------------\n",
        "def build_model(n1=128, n2=64, activation='relu', extra_layer=False):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "    # Hidden Layer 1\n",
        "    model.add(Dense(n1, activation=activation))\n",
        "\n",
        "    # Hidden Layer 2\n",
        "    model.add(Dense(n2, activation=activation))\n",
        "\n",
        "    # Optional: Tambah Hidden Layer 3\n",
        "    if extra_layer:\n",
        "        model.add(Dense(32, activation=activation))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Kompilasi model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# FUNGSI TRAIN + EVALUASI\n",
        "# -----------------------------\n",
        "def train_and_eval(title, **model_params):\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"Model: {title}\")\n",
        "    print(f\"Parameter: {model_params}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    model = build_model(**model_params)\n",
        "\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=128, verbose=0)\n",
        "    dur = time.time() - start\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Akurasi  : {acc:.4f}\")\n",
        "    print(f\"Waktu    : {dur:.2f} detik\")\n",
        "    return acc, dur"
      ],
      "metadata": {
        "id": "o0nVHswLOvkd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coba dengan beberapa parameter lain:\n",
        "\n",
        "#Ubah jumlah neuron di hidden layer (misal: 256 dan 128).\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=128, verbose=1)\n",
        "dur = time.time() - start\n",
        "\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(\"Akurasi:\", acc)\n",
        "print(\"Waktu training:\", dur, \"detik\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtSXBUo7Ob_l",
        "outputId": "0062a002-3ac9-4c96-e4d1-e93ad63acdd2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8546 - loss: 0.5120\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9686 - loss: 0.1044\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9788 - loss: 0.0663\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9866 - loss: 0.0439\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.9899 - loss: 0.0325\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9662 - loss: 0.1138\n",
            "Akurasi: 0.9707000255584717\n",
            "Waktu training: 23.539798259735107 detik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tambahkan satu hidden layer lagi.\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),   # Layer tambahan\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=128, verbose=1)\n",
        "dur = time.time() - start\n",
        "\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(\"Akurasi:\", acc)\n",
        "print(\"Waktu training:\", dur, \"detik\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lchs0IDyOjRD",
        "outputId": "e8f39404-6a7f-46f7-b9b5-ee24ec633737"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8484 - loss: 0.5207\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9676 - loss: 0.1104\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9775 - loss: 0.0719\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9852 - loss: 0.0471\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.9888 - loss: 0.0352\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9756 - loss: 0.0799\n",
            "Akurasi: 0.9787999987602234\n",
            "Waktu training: 25.748926401138306 detik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Eksperimen dengan fungsi aktivasi Sigmoid vs ReLU.\n",
        "\n",
        "#sigmoid\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(256, activation='sigmoid'),\n",
        "    Dense(128, activation='sigmoid'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=128, verbose=1)\n",
        "dur = time.time() - start\n",
        "\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(\"Akurasi:\", acc)\n",
        "print(\"Waktu training:\", dur, \"detik\")\n",
        "\n",
        "#relu\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=128, verbose=1)\n",
        "dur = time.time() - start\n",
        "\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(\"Akurasi:\", acc)\n",
        "print(\"Waktu training:\", dur, \"detik\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj1fMmSuOlCy",
        "outputId": "5ded72c2-1481-43f1-87c6-9cdfa504b5a4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7305 - loss: 1.0698\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9278 - loss: 0.2516\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9496 - loss: 0.1760\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9597 - loss: 0.1361\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9674 - loss: 0.1133\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9616 - loss: 0.1196\n",
            "Akurasi: 0.9667999744415283\n",
            "Waktu training: 23.85799241065979 detik\n",
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8578 - loss: 0.4883\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9675 - loss: 0.1091\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9796 - loss: 0.0667\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9846 - loss: 0.0494\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9895 - loss: 0.0329\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.0923\n",
            "Akurasi: 0.9782999753952026\n",
            "Waktu training: 21.6363263130188 detik\n"
          ]
        }
      ]
    }
  ]
}