{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxrjltmMn/bKV7Pf94SSeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trisskmasarahh/Machine-Learning_Ganjil_2025/blob/main/Praktikum_1_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 1"
      ],
      "metadata": {
        "id": "WC9pyHkL_SCo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQCKVngn9Iwo",
        "outputId": "c4ff9451-4d6d-4a1f-9f79-cd0820818dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.26982240673410274\n",
            "Epoch 1000, Loss: 0.24956199103011312\n",
            "Epoch 2000, Loss: 0.23381478073581946\n",
            "Epoch 3000, Loss: 0.16860630285887235\n",
            "Epoch 4000, Loss: 0.06575747350298303\n",
            "Epoch 5000, Loss: 0.020225053567813383\n",
            "Epoch 6000, Loss: 0.010093171321889773\n",
            "Epoch 7000, Loss: 0.0064177580716371346\n",
            "Epoch 8000, Loss: 0.004616079042232019\n",
            "Epoch 9000, Loss: 0.003569522847902657\n",
            "Prediksi:\n",
            "[[0.04612863]\n",
            " [0.9487803 ]\n",
            " [0.94887809]\n",
            " [0.0648922 ]]\n"
          ]
        }
      ],
      "source": [
        "#Langkah:\n",
        "\n",
        "#Buat dataset sederhana (XOR).\n",
        "#Inisialisasi bobot dan bias.\n",
        "#Implementasikan forward pass.\n",
        "#Hitung error dan lakukan backpropagation.\n",
        "#Update bobot menggunakan gradient descent.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ubah jumlah neuron hidden layer menjadi 3."
      ],
      "metadata": {
        "id": "Hxtx2VbV_1_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tugas 1:\n",
        "\n",
        "#Ubah jumlah neuron hidden layer menjadi 3.\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3  # diganti dari 2 â†’ 3 neuron\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Aktivasi sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "loss_history_3 = []\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    error = y - a2\n",
        "\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        loss_history_3.append(loss)\n",
        "        print(f\"[Hidden 3] Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "print(\"\\nPrediksi (hidden=3):\")\n",
        "print(a2.round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXlgKUPC-rOn",
        "outputId": "eeee00ba-a691-4e14-fe16-17f61c2ea485"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hidden 3] Epoch 0, Loss: 0.2542481957251896\n",
            "[Hidden 3] Epoch 1000, Loss: 0.25030913373915975\n",
            "[Hidden 3] Epoch 2000, Loss: 0.250075621030659\n",
            "[Hidden 3] Epoch 3000, Loss: 0.25000948248451094\n",
            "[Hidden 3] Epoch 4000, Loss: 0.24995747261221457\n",
            "[Hidden 3] Epoch 5000, Loss: 0.24986871625922982\n",
            "[Hidden 3] Epoch 6000, Loss: 0.2496137935926775\n",
            "[Hidden 3] Epoch 7000, Loss: 0.24831231699974896\n",
            "[Hidden 3] Epoch 8000, Loss: 0.2349810405742057\n",
            "[Hidden 3] Epoch 9000, Loss: 0.15184931217075504\n",
            "\n",
            "Prediksi (hidden=3):\n",
            "[[0.193]\n",
            " [0.852]\n",
            " [0.815]\n",
            " [0.164]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bandingkan hasil loss dengan konfigurasi awal."
      ],
      "metadata": {
        "id": "cpaIxsTIAKMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara keseluruhan, konfigurasi awal terbukti lebih superior dibandingkan konfigurasi dengan hidden layer 3. Konfigurasi awal mampu mencapai konvergensi yang lebih cepat dan menghasilkan nilai loss akhir yang jauh lebih kecil (0.003 berbanding 0.151). Hal ini menunjukkan bahwa arsitektur awal lebih mampu memetakan karakteristik data input dengan presisi tinggi tanpa mengalami hambatan belajar (stagnansi) seperti yang terjadi pada percobaan kedua."
      ],
      "metadata": {
        "id": "P07vrk_DF-iT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
      ],
      "metadata": {
        "id": "RZGJBVNrADOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tambahkan fungsi aktivasi ReLU dan bandingkan hasil.\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Aktivasi ReLU & Sigmoid\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "loss_history_relu = []\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    error = y - a2\n",
        "\n",
        "    # Backprop\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        loss_history_relu.append(loss)\n",
        "        print(f\"[ReLU] Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "print(\"\\nPrediksi (ReLU):\")\n",
        "print(a2.round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BdyioLm-15f",
        "outputId": "48183aa4-d0da-49a1-8ac6-ec25d925df35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ReLU] Epoch 0, Loss: 0.2800749885219016\n",
            "[ReLU] Epoch 1000, Loss: 0.25\n",
            "[ReLU] Epoch 2000, Loss: 0.25\n",
            "[ReLU] Epoch 3000, Loss: 0.25\n",
            "[ReLU] Epoch 4000, Loss: 0.25\n",
            "[ReLU] Epoch 5000, Loss: 0.25\n",
            "[ReLU] Epoch 6000, Loss: 0.25\n",
            "[ReLU] Epoch 7000, Loss: 0.25\n",
            "[ReLU] Epoch 8000, Loss: 0.25\n",
            "[ReLU] Epoch 9000, Loss: 0.25\n",
            "\n",
            "Prediksi (ReLU):\n",
            "[[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n"
          ]
        }
      ]
    }
  ]
}